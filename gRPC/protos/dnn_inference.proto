syntax = "proto3";

package dnn_surgery;

// Represents a tensor shape
message TensorShape {
    repeated int32 dimensions = 1;
}

// Represents a tensor
message Tensor {
    bytes data = 1;  // Serialized tensor data
    TensorShape shape = 2;
    string dtype = 3;  // Data type (float32, float64, etc.)
    bool requires_grad = 4;
}

// Request from edge device with intermediate tensor
message InferenceRequest {
    Tensor tensor = 1;
    string model_id = 2;  // Identifier for which model/layer to start from
}

// Response from cloud with final output
message InferenceResponse {
    Tensor tensor = 1;
    bool success = 2;
    string error_message = 3;
}

// Layer profiling metrics from client hardware
message LayerProfile {
    int32 layer_idx = 1;
    string layer_name = 2;
    double execution_time = 3;  // milliseconds
    int64 memory_usage = 4;     // bytes
    repeated int32 input_size = 5;
    repeated int32 output_size = 6;
    int64 data_transfer_size = 7;
    double computation_complexity = 8;
    double execution_time_std = 9;
    double min_execution_time = 10;
    double max_execution_time = 11;
}

// System information from client
message SystemInfo {
    string model = 1;
    string cpu_model = 2;
    int32 cpu_cores = 3;
    int32 memory_total_mb = 4;
    string architecture = 5;
    string os_info = 6;
    string python_version = 7;
    string pytorch_version = 8;
    string timestamp = 9;
}

// Complete client profiling data
message ClientProfile {
    string model_name = 1;
    SystemInfo system_info = 2;
    repeated LayerProfile layer_metrics = 3;
    int32 num_samples = 4;
    repeated int32 input_size = 5;
    double total_model_time = 6;
    double avg_layer_time = 7;
    int32 total_layers = 8;
}

// Request to send profiling data to server
message ProfilingRequest {
    ClientProfile profile = 1;
    string client_id = 2;  // Unique identifier for client
}

// Response after receiving profiling data
message ProfilingResponse {
    bool success = 1;
    string message = 2;
    string updated_split_config = 3;  // Optional: new split configuration
}

// Service definition for DNN inference
service DNNInference {
    // Send intermediate tensor to cloud for further processing
    rpc ProcessTensor (InferenceRequest) returns (InferenceResponse) {}
    
    // Send client profiling data for optimal split calculation
    rpc SendProfilingData (ProfilingRequest) returns (ProfilingResponse) {}
}
